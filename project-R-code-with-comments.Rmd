---
title: "Project R Code with Comments"
output: 
    html_document: 
       keep_md: true
---

## Section 1: Installing Keras

```{r eval=FALSE, include=TRUE}
#Installing Keras
install.packages("devtools")
devtools::install_github("rstudio/keras")
install_keras()
```

```{r}
#Loading Keras library
library(keras)
```

## Section 2: Loading the dataset

```{r}
data <- read.csv('./creditcard.csv', stringsAsFactors = F)
head(data)
```

## Section 3: Data Cleaning

### Section 3.1: Verifying the number of classes

```{r}
#Printing distinct values in the feature Class
unique(data$Class)
```

### Section 3.2: Checking the timestamps to count the number of days they correspond to

```{r}
#Record the smallest timestamp in the dataset
first_timestamp <- min(data$Time)

#Record the last timestamp in the dataset
last_timestamp <- max(data$Time)

#Find the difference between the first timestamp in the dataset and the last
#timestamp in the dataset
last_timestamp - first_timestamp
```

Since the elapsed seconds between the first transaction and the last transaction in the dataset are less than 172,800, which is the number of seconds in two days, then the transactions in the dataset all occurred within two days. This verifies what is in the dataset description on Kaggle.

### Section 3.3: Checking if there are any missing values in the dataset 

```{r}
#We know that all the features in our dataset are numeric values. In R, the arithmetic #function on missing values returns an NA. So we will use the arithmetic mean 
#function to check if there are any missing values in the dataset

arithmetic_mean <- sapply(data, FUN=mean)
arithmetic_mean
```

There were no returned NA values. Therefore, we don't have any missing values in our dataset. 


## Section 4: Exploratory data analysis

### Section 4.1: Looking into classes distribution 

```{r}
#Loading ggplot2 and dplyr libraries
library(ggplot2)
library(dplyr)
```

```{r eval=FALSE, include=TRUE}
#Plotting the count of each class on a bar chart
data %>% ggplot(aes(Class))+geom_bar()
```

### Section 4.2: Correlation matrix and correlogram 

```{r}
#Calculating the correlation matrix 
data.cor = cor(data)
```

```{r eval=FALSE, include=TRUE}
#Installing the corrplot package
install.packages("corrplot")
```

```{r}
#Loading the carrplot library
library(corrplot)
```

```{r eval=FALSE, include=TRUE}
#Plotting a correlogram
corrplot(data.cor)
```


### Section 4.3: The pattern between time and the transaction amount 

```{r eval=FALSE, include=TRUE}
#Plotting transaction amount versus time by class 
data %>% ggplot(aes(Time,Amount))+geom_point()+facet_grid(Class~.)
```

### Section 4.4: Filtering out transactions less than 400 and plotting transaction amounts by class

```{r eval=FALSE, include=TRUE}
data$Class <- as.factor(data$Class)
data %>% filter(Amount<400) %>% ggplot(aes(Class,Amount))+geom_violin()
```

## Section 5: Data Analysis

### Section 5.1: Splitting the dataset into a training set and a testing set. Also, dropping Time and Class features from the training set. 

```{r}
#Setting aside 20% of the data as a testing set and 80% as a training set
index <- sample(nrow(data), size=0.2*nrow(data))
training_set <- data[-index,]
testing_set <- data[index,]

#Setting the response variable for both the testing and training sets. This will be used later #to evaluate the model performance
y_training_set <- training_set$Class
y_testing_set <- testing_set$Class

#Excluding labels (Time and Class) from the X_training and X_testing sets
X_training_set <- training_set %>% select(-one_of(c("Time","Class")))
X_testing_set <- testing_set %>% select(-one_of(c("Time","Class")))

#Turning the data frame into a matrix 
X_training_set <- as.matrix(X_training_set)
X_testing_set <- as.matrix(X_testing_set)
```


### Section 5.2: The first autoencoder neural network architecture
```{r}
#Based on our dataset the input and output dimensions will be 29. We will just define one #variable and use it for both the outer layer and the inner layer
input_layer_dimension <- 29

#Defining the number of neurons in the outer layer
neurons_in_outer_layer <- 15

#Defining the number of neurons in the inner layer
neurons_in_inner_layer <- 10

#Defining the input layer of the network which is also the first encoder layer
input_layer <- layer_input(shape=c(input_layer_dimension))

#Defining the encoder of the network and using ReLU as an activation function
encoder <- layer_dense(units=neurons_in_outer_layer,activation='relu')(input_layer)
encoder <- layer_dense(units=neurons_in_inner_layer, activation='relu')(encoder)

#Transation from the encoder which is the first decoder layer
decoder <- layer_dense(units=neurons_in_inner_layer)(encoder)

#Defining decoder layers
decoder <- layer_dense(units=neurons_in_outer_layer)(decoder)
decoder <- layer_dense(units=input_layer_dimension)(decoder)

#Defining the neural network using the encoders and decoders we specified above
autoencoder_arch1 <- keras_model(inputs=input_layer, outputs = decoder)
```

```{r}
#Compiling the model using mean square error as a loss function and accuracy as a metics
autoencoder_arch1 %>% compile(optimizer='adam',loss='mean_squared_error',
                              metrics=c('accuracy'))

#Fitting the model and saving the results in the variable "steps"
#Here we specified the batch size as 30 and the number of epochs as 10
#Also, the validation split is set to 0.2
steps  <- autoencoder_arch1 %>% fit(X_training_set,X_training_set, epochs = 10, batch_size = 30, validation_split=0.2)
```

```{r eval=FALSE, include=TRUE}
#Plotting the results to show the accuracy and loss values for each epoch
plot(steps)
```

### Section 5.3: Making predictions using the trained network and test data, defining the threshold for reconstruction errors, and plotting the precision-recall curve.

```{r}
#Making predictions using the trained network and test data
predictions <- autoencoder_arch1 %>% predict(X_testing_set)
predictions <- as.data.frame(predictions)
```

```{r}
#Setting the threshold for the reconstruction errors to 40
y_predictions <- ifelse(rowSums((predictions - X_testing_set)**2)/40<1,rowSums((predictions - X_testing_set)**2)/40,1)
```

```{r eval=FALSE, include=TRUE}
#Plotting the precision-recall curve
library(ROCR)
predic <- prediction(y_predictions, y_testing_set)
perform <- performance(predic, measure = "tpr", x.measure = "fpr")
plot(perform, col='blue')
```

### Section 5.4: The second autoencoder neural network architecture

```{r}
#Based on our dataset the input and output dimensions will be 29. We will just define one #variable and use it for both the outer layer and the inner layer
input_layer_dimension <- 29

#Defining the number of neurons in each inner layer
neurons_in_first_inner_layer <- 20
neurons_in_second_inner_layer <- 12
neurons_in_third_inner_layer <- 6
neurons_in_fourth_inner_layer <- 3

#Defining the input layer of the network
input_layer <- layer_input(shape=c(input_layer_dimension))

#Defining the encoder of the network and using ReLU as an activation function
encoder <- layer_dense(units=neurons_in_first_inner_layer,activation='relu')(input_layer)
encoder <- layer_dense(units=neurons_in_second_inner_layer, activation='relu')(encoder)
encoder <- layer_dense(units=neurons_in_third_inner_layer, activation='relu')(encoder)
encoder <- layer_dense(units=neurons_in_fourth_inner_layer, activation='relu')(encoder)

#The first layer of the decoder
decoder <- layer_dense(units=neurons_in_fourth_inner_layer)(encoder)

#Defining the decoder layers
decoder <- layer_dense(units=neurons_in_third_inner_layer)(decoder)
decoder <- layer_dense(units=neurons_in_second_inner_layer)(decoder)
decoder <- layer_dense(units=neurons_in_first_inner_layer)(decoder)
decoder <- layer_dense(units=input_layer_dimension)(decoder)

#Defining the neural network using the encoders and decoders we specified above
autoencoder_arch2 <- keras_model(inputs=input_layer, outputs = decoder)
```

```{r}
#Compiling the model using mean square error as a loss function and accuracy as a metics
autoencoder_arch2 %>% compile(optimizer='adam',loss='mean_squared_error',
                              metrics=c('accuracy'))

#Fitting the model and saving the results in the variable "steps".
#Here we specified the batch size as 30 and the number of epochs as 10
#Also, the validation split is set to 0.2 
steps_2  <- autoencoder_arch2 %>% fit(X_training_set,X_training_set, epochs = 10, batch_size = 30, validation_split=0.2)
```

```{r eval=FALSE, include=TRUE}
#Plotting the results to show the accuracy and loss values for each epoch
plot(steps_2)
```

### Section 5.5: Making predictions using the trained network and test data, defining the threshold for reconstruction errors, and plotting the precision-recall curve.

```{r}
#Making predictions using the trained network and test data
predictions_2 <- autoencoder_arch2 %>% predict(X_testing_set)
predictions_2 <- as.data.frame(predictions_2)
```

```{r}
#Setting the threshold for the reconstruction errors to 40
y_predictions_2 <- ifelse(rowSums((predictions_2 - X_testing_set)**2)/40<1,rowSums((predictions_2 - X_testing_set)**2)/40,1)
```

```{r eval=FALSE, include=TRUE}
#Plotting the precision-recall curve
library(ROCR)
predic_2 <- prediction(y_predictions_2, y_testing_set)
perform_2 <- performance(predic_2, measure = "tpr", x.measure = "fpr")
plot(perform_2, col='blue')
```
